import json
import torch
import logging

logger = logging.getLogger(__name__)

def build_optimizer_parameters(config, model):
    # L·∫•y Learning Rate g·ªëc t·ª´ config
    base_lr = float(config.OPTIMIZER.lr)
    
    # H·ªá s·ªë tƒÉng t·ªëc cho c√°c l·ªõp m·ªõi (Vision Proj, Classifier...)
    # C√°c l·ªõp n√†y c·∫ßn h·ªçc nhanh h∆°n Backbone (Swin/PhoBERT)
    HEAD_LR_MULT = 10.0 

    # Danh s√°ch c√°c t·ª´ kh√≥a ƒë·ªÉ nh·∫≠n di·ªán l·ªõp m·ªõi (c·∫ßn h·ªçc nhanh)
    # 'vision_proj': l·ªõp c·∫ßu n·ªëi b·∫°n m·ªõi th√™m
    # 'head', 'classifier', 'cls': c√°c l·ªõp ƒë·∫ßu ra
    # 'adapter': n·∫øu c√≥ d√πng adapter
    head_keywords = ['vision_proj', 'classifier', 'head', 'cls', 'adapter']

    # Danh s√°ch c√°c tham s·ªë kh√¥ng √°p d·ª•ng Weight Decay (chu·∫©n chung)
    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight', 'pos_embed', 'relative_position_bias_table']

    if "weight_decay" in config.TRAINING.keys():
        weight_decay = config.TRAINING["weight_decay"]
    else:
        weight_decay = 0.01

    # Chia tham s·ªë th√†nh 4 nh√≥m:
    # 1. Backbone (Swin/PhoBERT) - C√≥ Decay
    # 2. Backbone (Swin/PhoBERT) - Kh√¥ng Decay (bias, layernorm...)
    # 3. New Layers (VisionProj/Head) - C√≥ Decay (H·ªçc nhanh g·∫•p 10 l·∫ßn)
    # 4. New Layers (VisionProj/Head) - Kh√¥ng Decay (H·ªçc nhanh g·∫•p 10 l·∫ßn)
    
    backbone_decay = []
    backbone_no_decay = []
    head_decay = []
    head_no_decay = []

    # Duy·ªát qua to√†n b·ªô tham s·ªë c·ªßa m√¥ h√¨nh
    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue
            
        # B·ªè qua pooler c·ªßa BERT n·∫øu kh√¥ng c·∫ßn thi·∫øt (nh∆∞ code c≈© c·ªßa b·∫°n)
        if 'pooler' in name:
            continue

        # Ki·ªÉm tra xem tham s·ªë n√†y thu·ªôc nh√≥m HEAD (L·ªõp m·ªõi) hay BACKBONE (C≈©)
        is_head = any(k in name for k in head_keywords)
        
        # Ki·ªÉm tra xem c√≥ √°p d·ª•ng Weight Decay kh√¥ng
        is_no_decay = any(nd in name for nd in no_decay)

        if is_head:
            if is_no_decay:
                head_no_decay.append(param)
            else:
                head_decay.append(param)
        else:
            if is_no_decay:
                backbone_no_decay.append(param)
            else:
                backbone_decay.append(param)

    # T·∫°o danh s√°ch optimizer grouped parameters
    optimizer_grouped_parameters = [
        # Nh√≥m 1: Backbone (LR chu·∫©n)
        {
            'params': backbone_decay,
            'weight_decay': weight_decay,
            'lr': base_lr
        },
        {
            'params': backbone_no_decay,
            'weight_decay': 0.0,
            'lr': base_lr
        },
        # Nh√≥m 2: New Layers (LR nh√¢n l√™n 10 l·∫ßn)
        {
            'params': head_decay,
            'weight_decay': weight_decay,
            'lr': base_lr * HEAD_LR_MULT
        },
        {
            'params': head_no_decay,
            'weight_decay': 0.0,
            'lr': base_lr * HEAD_LR_MULT
        }
    ]
    
    # In th√¥ng tin ƒë·ªÉ ki·ªÉm tra (Debug)
    print(f"üî• OPTIMIZER SETUP:")
    print(f"   - Base LR (Backbone): {base_lr}")
    print(f"   - Head LR (New Layers): {base_lr * HEAD_LR_MULT}")
    print(f"   - Backbone params: {len(backbone_decay) + len(backbone_no_decay)}")
    print(f"   - Head params (High LR): {len(head_decay) + len(head_no_decay)}")

    return optimizer_grouped_parameters